# ELK Stack (Elastic Stack) - Elasticsearch / Logstash / Kibana

## Logstash

### Install

Logstash is installed by downloading and then unpacking to a convenient directory.  Full details are given in the Logstash [documentation](https://www.elastic.co/guide/en/logstash/current/installing-logstash.html)

Once it has been unpacked and is ready to run, some basic configuration is required.

### Basic JSON Configuration

The simplest way to use Logstash is to generate a configuration which can read the JSON log files generated by the application. To do this, create a file `logstash.conf` containing the 
following and copy it to the where you unpacked Logstash.

```
input {
  file {
    path => ["<your log directory goes here>/sywtwam.json"]
    codec =>   json {
      charset => "UTF-8"
    }
  }
}

output {
   stdout { codec => rubydebug }
}

```

Now execute `bin/logstash -f logstash.conf` to start Logstash.  If you start the application, any logging to sywtwam.json will be displayed on the terminal.  This will be directed to Elasticsearch 
in a later step.

### Flexible Configuration Using Grok

An more flexible way to process log information is to read the raw logs themselves and extract fields via the use of grep.  For example, use the following `logstash.conf` configuration:
```
input {
  file {
    path => ["<your log directory goes here>/sywtwam.log"]
    codec =>   plain {
      charset => "UTF-8"
    }
  }
}


filter {
   # valid log line ?
   grok {
      match => [ "message", "%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{WORD:level}%{SPACE}%{NUMBER:PID}%{SPACE}---%{SPACE}%{SYSLOG5424SD:threadname}%{SPACE}%{SYSTEM:system}%{SPACE}%{SUBSYSTEM:subsystem}%{SPACE}%{JAVACLASSSHORT:class}%{SPACE}:%{SPACE}%{GREEDYDATA:msg}" ]
      break_on_match => true
  }
  
  # if invalid, delete
  if "_grokparsefailure" in [tags] { drop {} }

  # use the log file timestamp, not the logstash one
  date {
	match => ["timestamp", "ISO8601"]
	target => "@timestamp"
	}

  # delete unwanted fields
  mutate {
      remove_field => ["message","path","timestamp"]
    }
}

output {
   stdout { codec => rubydebug }
}
```

Before you can use this, you will need to add definitions for the non-standard patterns _SYSTEM, SUBSYSTEM_ and _JAVACLASSSHORT_.  These are described to Logstash 
by defining a file `patterns.txt` under the `/patterns` directory with the following content:

```text
JAVACLASSSHORT (?:[\.]?[a-zA-Z0-9-]+\.)*[A-Za-z0-9$]+
SYSTEM [a-zA-Z0-9._-]+
SUBSYSTEM [a-zA-Z0-9._-]+
```

The filter attempts to analyze each line to verify that it matches the expected format. If it does, the labelled fields are populated for later use. 
If no match is found then an additional field `_grokparsefailure` is generated. The `if` clause then directs the filter to discard the line.  This guarantees that only valid log entries are processed.

Refer to the Logstash [documentation](https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html) which shows in detail how to manipulate field information.

It can be difficult to generate correct grok expressions.  Debugging is made significantly simpler by using the [Grok Debugger](https://grokdebug.herokuapp.com/)

### Network Logging

If reading local log files is not a desired solution, then it is possible to send / receive logs directly across the network.  If we change the *input* step to the following we can
listen for log data.

```
input {
  tcp {
     port => 9500
     host => "localhost"
     codec => plain {
	charset => "UTF-8"
	}
   }
}
```

This allows Logstash to be either situated with the application (e.g. in the same conatiner) or anywhere else across the network which opens up the possible
use of multiple instances / load balancers etc


## Elasticsearch

Elasticsearch is a distributed, RESTful search and analytics engine based on [Lucene](https://lucene.apache.org/). It is used in the ELK stack as the repository of log information for search, 
analysis and visualization tasks from Kibana.

### Install

Elasticsearch is installed by downloading and then unpacking to a convenient directory.  Full details are given in the Elasticsearch [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/_installation.html)

Once it has been unpacked and is ready to run, we need to describe the log file data being.  This is done by setting a mapping associated to an index.  The index in this instance is `sywtwam_idx`.  We also 
need to tell Logstash to associate log records with this index.

To generate the mapping, start Elasticsearch and execute the following curl command:

```bash
curl -XPUT 'localhost:9200/sywtwam_idx?pretty' -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "sywtwam": {
      "properties": {
        "@timestamp": {
          "type": "date",
          "format": "yyyy-MM-dd'\''T'\''HH:mm:ss.SSSZZ"
        },
        "level": {
          "type": "keyword"
        },
        "threadname": {
          "type": "text"
        },
        "PID": {
          "type": "long"
        },
        "system": {
          "type": "keyword"
        },
        "subsystem": {
          "type": "keyword"
        },
        "class": {
          "type": "keyword"
        },
        "msg": {
          "type": "text"
        }
      }
    }
  }
}
'

```

The Logstash output step needs to be updated to:

````text
output {
   elasticsearch { hosts => "localhost:9200" index=>"sywtwam_idx" document_type => "logs"}
   stdout { codec => rubydebug }
}
````
This will send logs to Elasticsearch as well as display them on the console.

## Kibana

Now we have log data saved away into Elasticsearch, we need to install and configure Kibana to visualize this.

### Install

Kibana is installed by downloading and then unpacking to a convenient directory.  Full details are given in the Kibana [documentation](https://www.elastic.co/guide/en/kibana/current/install.html).

On first execution of Kibana, we need to identify the index pattern being used, in this case `sywtwam_idx`.  This is done by going to the Management -> Index Patterns page, and entering `sywtwam_idx` 
into the *index name or pattern* box.  The *Time Filter field name* should auto-populate to `@timestamp`. Press *Create* to finish installation.

## Useful Links

| Action | Link |
|----------------|-----------------|
| Kibana | http://localhost:5601
| Kibana health page | http://localhost:5601/status
| List Indexes | curl -XGET 'localhost:9200/_cat/indices?v&pretty'
| Delete index | curl -XDELETE 'localhost:9200/sywtwam_idx?pretty&pretty'


