# ELK Stack (Elastic Stack) - Elasticsearch / Logstash / Kibana



## Logstash

### Install

Logstash is installed by downloading and then unpacking to a convenient directoy.  Full details are given in the Logstash [documentation](https://www.elastic.co/guide/en/logstash/current/installing-logstash.html)

Once it has been unpacked and is ready to run, some basic configuration is required.

### Basic JSON Configuration

The simplest way to use Logstash is to generate a configuration which can read the JSON log files generated by the application. To do this, create a file `logstash.conf` containing the 
following and copy it to the where you unpacked Logstash.

```
input {
  file {
    path => ["<your log directory goes here>/sywtwam.json"]
    codec =>   json {
      charset => "UTF-8"
    }
  }
}

output {
   stdout { codec => rubydebug }
}

```

Now execute `bin/logstash -f logstash.conf` to start Logstash.  If you start the application, any logging to sywtwam.json will be displayed on the terminal.  This will be directed to Elasticsearch 
in a later step.

### Flexible Configuration Using Grok

An more flexible way to process log information is to read the raw logs themselves and extract fields via the use of grep.  For example, use the following `logstash.conf` configuration:
```
input {
  file {
    path => ["<your log directory goes here>/sywtwam.log"]
    codec =>   plain {
      charset => "UTF-8"
    }
  }
}


filter {
   # valid log line ?
   grok {
      match => [ "message", "%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{WORD:level}%{SPACE}%{NUMBER:PID}%{SPACE}---%{SPACE}%{SYSLOG5424SD:threadname}%{SPACE}%{SYSTEM:system}%{SPACE}%{SUBSYSTEM:subsystem}%{SPACE}%{JAVACLASSSHORT:class}%{SPACE}:%{SPACE}%{GREEDYDATA:msg}" ]
      tag_on_failure => true
      break_on_match => true
  }
  # if invalid, delete
  if "_grokparsefailure" in [tags] { drop {} }

  # use the log file timestamp, not the logstash one
  date {
	match => ["timestamp", "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
	target => "@timestamp"
	}

  # delete unwanted fields
  mutate {
      remove_field => ["message","path","timestamp"]
    }
}


output {
   stdout { codec => rubydebug }
}
```

Before you can use this, you will need to add definitions for the non standard patterns _SYSTEM, SUBSYSTEM_ and _JAVACLASSSHORT_.  These are described to Logstash 
by defining a file `patterns.txt` under the `/patterns` directory with the following content:

```text
JAVACLASSSHORT (?:[\.]?[a-zA-Z0-9-]+\.)*[A-Za-z0-9$]+
SYSTEM [a-zA-Z0-9._-]+
SUBSYSTEM [a-zA-Z0-9._-]+
```

The filter attempts to analyze each line to verify that it matches the expected format. If it does, the labelled fields are populated for later use. 
If no match is found then an additional field `_grokparsefailure` is generated. The `if` clause then directs the filter to discard the line.  This guarantees that only valid log entries are processed.

Refer to the Logstash [documentation](https://www.elastic.co/guide/en/logstash/current/advanced-pipeline.html) which shows in detail how to manipulate field information.

It can be difficult to generate correct grok expressions.  Debugging is made significantly simpler by using the [Grok Debugger](https://grokdebug.herokuapp.com/)

## Elasticsearch

### Install


## Kibana

### Install

